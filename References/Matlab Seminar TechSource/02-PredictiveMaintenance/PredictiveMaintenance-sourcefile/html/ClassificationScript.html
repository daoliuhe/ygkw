
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Data Analytics for Predictive Maintenance</title><meta name="generator" content="MATLAB 9.0"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2016-03-11"><meta name="DC.source" content="ClassificationScript.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Data Analytics for Predictive Maintenance</h1><!--introduction--><p>We have a fleet of 100 aircraft engines of the same model. Each engine starts with different unknown degrees of initial wear and manufacturing variation which is considered normal. Each engine is operating at normal conditions at the beginning of the time series but degrades over time until a predefined, unknown failure threshold is reached. The objective is to predict from any point in time how long we have until we need to perform maintenance.</p><p>References:</p><div><ul><li>A. Saxena, K. Goebel, D. Simon, and N. Eklund, &#8220;Damage Propagation Modeling for Aircraft Engine Run-to-Failure Simulation&#8221;, in the Proceedings of the 1st International Conference on Prognostics and Health Management (PHM08), Denver CO, Oct 2008.</li></ul></div><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Load data</a></li><li><a href="#2">Define classification thresholds</a></li><li><a href="#3">Visualize all sensor data in categories</a></li><li><a href="#4">Rapid preparation for Machine Learning by leveraging App generated codes</a></li><li><a href="#5">Create training and test sets</a></li><li><a href="#6">Easy to develop, evaluate models by leveraging Apps, e.g., k-nearest neighbor</a></li><li><a href="#7">Cost Matrix</a></li><li><a href="#8">New model with K=5 and custom cost matrix</a></li></ul></div><h2>Load data<a name="1"></a></h2><p>We explored importing, visualizing and preprocessing the data in the UnsupervisedScript, so here we will simply load in the data.</p><pre class="codeinput">load <span class="string">classificationData</span>
</pre><h2>Define classification thresholds<a name="2"></a></h2><p>To solve this as a classification problem, we need to define what the classes are and where the boundaries are between them. This typically is something you cannot do purely from the equipment sensor data. Here we drew arbitrary boundaries to create four different classes. We will attempt to classify each point as being urgently in need of maintenance, or having a short, medium, or long time until maintenance is needed.</p><pre class="codeinput">catThreshold = [50,125,200];
orderedCategory = {<span class="string">'urgent'</span>,<span class="string">'short'</span>,<span class="string">'medium'</span>,<span class="string">'long'</span>};
fullDataset.TTF = createThresholds(fullDataset,catThreshold,orderedCategory);

<span class="comment">% Look at spread of Classes</span>
tabulate(fullDataset.TTF)

<span class="comment">% Pull out just the sensor data</span>
sensorData = fullDataset(:,3:end); <span class="comment">% ignore Unit and Time variables</span>
</pre><pre class="codeoutput">   Value    Count   Percent
  urgent     5000     24.71%
   short     7499     37.07%
  medium     5903     29.18%
    long     1829      9.04%
</pre><h2>Visualize all sensor data in categories<a name="3"></a></h2><p>Here we look at a subset of the available sensors. The engines have been aligned so that they all fail at time zero and negative values represent flights prior to failure. We can see how the sensors change as they approach failure and where we have placed the different classes.</p><pre class="codeinput">figure
<span class="keyword">for</span> ii = 1:9
    h(ii) = subplot(3,3,ii); <span class="comment">%#ok</span>
    scatter(h(ii),fullDataset.Time,fullDataset{:,2+ii},[],fullDataset.TTF, <span class="string">'filled'</span>);
    title(h(ii),fullDataset.Properties.VariableNames{2+ii})
    xlabel(h(ii),<span class="string">'Time'</span>)
    set(h(ii), <span class="string">'CLim'</span>, [1 length(catThreshold)+1])
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="ClassificationScript_01.png" style="width:560px;height:420px;" alt=""> <h2>Rapid preparation for Machine Learning by leveraging App generated codes<a name="4"></a></h2><p>One of the biggest challenges in machine learning is that there is no good way to know ahead of time what machine learning algorithm will work best until you try them out and see. The Classification Learner App from the Statistics and Machine Learning Toolbox makes greatly simplifies this workflow. We can explore many different techniques in the app, and in this case find that a K-nearest neighbors algorithm seems to work best.</p><pre class="codeinput"><span class="comment">% This command will bring up the app, or you can find it in the apps tab.</span>
<span class="comment">% Use the sensorData variable when importing data from the workspace into</span>
<span class="comment">% the Classification Learner App</span>
classificationLearner
</pre><h2>Create training and test sets<a name="5"></a></h2><p>The app will automatically create training and testing data sets when working within it. We can also create them ourselves when running outside the app.</p><pre class="codeinput">predictorNames = fullDataset.Properties.VariableNames(3:end-1);
predictors = fullDataset(:, predictorNames);
response = fullDataset.TTF;

<span class="comment">% Set up partitions and holdout validation</span>
cvp = cvpartition(response, <span class="string">'Holdout'</span>, 0.2);
Xtrain = predictors(cvp.training,:);
Ytrain = response(cvp.training,:);
disp(<span class="string">'Training set'</span>);
tabulate(Ytrain)

<span class="comment">% Test data</span>
Xtest = predictors(cvp.test,:);
Ytest = response(cvp.test,:);
disp(<span class="string">'Test set'</span>);
tabulate(Ytest)
</pre><pre class="codeoutput">Training set
   Value    Count   Percent
  urgent     4000     24.71%
   short     5999     37.07%
  medium     4722     29.18%
    long     1464      9.05%
Test set
   Value    Count   Percent
  urgent     1000     24.72%
   short     1500     37.07%
  medium     1181     29.19%
    long      365      9.02%
</pre><h2>Easy to develop, evaluate models by leveraging Apps, e.g., k-nearest neighbor<a name="6"></a></h2><p>In our case, k-nearest neighbor outperforms other models based on confusion matrix. From the app we can auto-generate MATLAB code to train the model, which is copied here.</p><pre class="codeinput">KNNClassifier = fitcknn(<span class="keyword">...</span>
    Xtrain, <span class="keyword">...</span>
    Ytrain, <span class="keyword">...</span>
    <span class="string">'Distance'</span>, <span class="string">'Euclidean'</span>, <span class="keyword">...</span>
    <span class="string">'Exponent'</span>, [], <span class="keyword">...</span>
    <span class="string">'NumNeighbors'</span>, 5, <span class="keyword">...</span>
    <span class="string">'DistanceWeight'</span>, <span class="string">'Equal'</span>, <span class="keyword">...</span>
    <span class="string">'Standardize'</span>, true, <span class="keyword">...</span>
    <span class="string">'ClassNames'</span>, orderedCategory);

<span class="comment">% Predict Response using Predictors in Test dataset</span>
validationPredictions = predict(KNNClassifier, Xtest);
validationPredictions = categorical(validationPredictions, orderedCategory,<span class="string">'Ordinal'</span>,true);

<span class="comment">% Visualize confusion matrix</span>
addpath(fullfile(pwd,<span class="string">'helperFunctions'</span>))
C_nn = confusionmat(Ytest,validationPredictions);
C_np = fdispConfusion(C_nn, <span class="string">'K-nearest neighbor (K=5)'</span>, orderedCategory);

figure
fheatmap(C_np, orderedCategory, orderedCategory, <span class="keyword">...</span>
    <span class="string">'%0.1f%%'</span>, <span class="keyword">...</span>
    <span class="string">'TickAngle'</span>, 45, <span class="keyword">...</span>
    <span class="string">'Colormap'</span>, <span class="string">'money'</span>, <span class="keyword">...</span>
    <span class="string">'Colorbar'</span>, true, <span class="keyword">...</span>
    <span class="string">'ColorLevels'</span>, 5);
title(<span class="string">'Confusion Matrix for K-Nearest Neighbor (K=5)'</span>);
xlabel(<span class="string">'Predicted class'</span>);
ylabel(<span class="string">'True class'</span>);
</pre><pre class="codeoutput">Performance of model K-nearest neighbor (K=5):
                     Predicted urgent    Predicted short    Predicted medium    Predicted long
    Actual urgent    92.50% (925)         6.70% (67)         0.80% (8)           0.00% (0)    
    Actual short      3.07% (46)         85.07% (1276)      11.27% (169)         0.60% (9)    
    Actual medium     1.02% (12)         14.56% (172)       80.44% (950)         3.98% (47)   
    Actual long       0.27% (1)          14.79% (54)        26.58% (97)         58.36% (213)  
</pre><img vspace="5" hspace="5" src="ClassificationScript_02.png" style="width:560px;height:420px;" alt=""> <h2>Cost Matrix<a name="7"></a></h2><p>We can use a cost matrix to prioritize certain errors. For example the 'urgent' cases are the most important to capture, so we can penalize errors when the 'urgent' class is misclassified more than other types of errors. However, it is also wasteful to do maintenance well before it is needed, so we may add a smaller penalty to when we predict 'urgent' when it is acutally 'medium' or 'long'.</p><pre class="codeinput">CostMatrix = ones(4) - eye(4);
CostMatrix(1,2:4) = 5;
CostMatrix(4,1) = 3;
CostMatrix(3,1) = 2;
disp(<span class="string">'Cost Matrix:'</span>)
disp(CostMatrix)
</pre><pre class="codeoutput">Cost Matrix:
     0     5     5     5
     1     0     1     1
     2     1     0     1
     3     1     1     0
</pre><h2>New model with K=5 and custom cost matrix<a name="8"></a></h2><p>We can use cost matrix to prioritize certain errors. Now we are optimizing for the overall impact of our mistakes based on our cost matrix rather than just minimizing the raw number of mistakes. We can see that this greatly reduces the number of urgent cases that are not correctly captured, but also increases the number of short cases that are incorrectly classified as urgent. This may actually cause us to make a greater number of total mistakes, but if it concentrates those mistakes in areas where they are less costly to make then that is preferrable.</p><pre class="codeinput"><span class="comment">% New model with K=5 and custom cost matrix</span>
KNNClassifier5c = fitcknn(<span class="keyword">...</span>
    Xtrain, <span class="keyword">...</span>
    Ytrain, <span class="keyword">...</span>
    <span class="string">'Distance'</span>, <span class="string">'Euclidean'</span>, <span class="keyword">...</span>
    <span class="string">'Exponent'</span>, [], <span class="keyword">...</span>
    <span class="string">'NumNeighbors'</span>, 5, <span class="keyword">...</span>
    <span class="string">'DistanceWeight'</span>, <span class="string">'Equal'</span>, <span class="keyword">...</span>
    <span class="string">'Standardize'</span>, true, <span class="keyword">...</span>
    <span class="string">'Cost'</span>,CostMatrix, <span class="keyword">...</span>
    <span class="string">'ClassNames'</span>, orderedCategory);

<span class="comment">% Predict Response using Predictors in Test dataset</span>
[validationPredictions, validationScores] = predict(KNNClassifier5c, Xtest);
validationPredictions = categorical(validationPredictions, orderedCategory,<span class="string">'Ordinal'</span>,true);

<span class="comment">% Visualize confusion matrix</span>
addpath(fullfile(pwd,<span class="string">'helperFunctions'</span>))
C_nn5c = confusionmat(Ytest,validationPredictions);
C_np5c = fdispConfusion(C_nn5c, <span class="string">'k-nearest neighbor (k=5) with CostMatrix'</span>, orderedCategory);

figure
fheatmap(C_np5c, orderedCategory, orderedCategory, <span class="keyword">...</span>
    <span class="string">'%0.1f%%'</span>, <span class="keyword">...</span>
    <span class="string">'TickAngle'</span>, 45, <span class="keyword">...</span>
    <span class="string">'Colormap'</span>, <span class="string">'money'</span>, <span class="keyword">...</span>
    <span class="string">'Colorbar'</span>, true, <span class="keyword">...</span>
    <span class="string">'ColorLevels'</span>, 5);
title(<span class="string">'Confusion Matrix for K-Nearest Neighbor (K=5) with CostMatrix'</span>);
xlabel(<span class="string">'Predicted class'</span>);
ylabel(<span class="string">'True class'</span>);
</pre><pre class="codeoutput">Performance of model k-nearest neighbor (k=5) with CostMatrix:
                     Predicted urgent    Predicted short    Predicted medium    Predicted long
    Actual urgent    98.70% (987)         0.50% (5)          0.80% (8)           0.00% (0)    
    Actual short     19.13% (287)        69.00% (1035)      11.27% (169)         0.60% (9)    
    Actual medium     2.46% (29)         13.38% (158)       80.19% (947)         3.98% (47)   
    Actual long       0.27% (1)          14.79% (54)        26.58% (97)         58.36% (213)  
</pre><img vspace="5" hspace="5" src="ClassificationScript_03.png" style="width:560px;height:420px;" alt=""> <p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2016a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Data Analytics for Predictive Maintenance
%
% We have a fleet of 100 aircraft engines of the same model. Each engine
% starts with different unknown degrees of initial wear and manufacturing
% variation which is considered normal. Each engine is operating at normal
% conditions at the beginning of the time series but degrades over time
% until a predefined, unknown failure threshold is reached. The objective
% is to predict from any point in time how long we have until we need to
% perform maintenance.
%
% References:
%
% * A. Saxena, K. Goebel, D. Simon, and N. Eklund, “Damage Propagation
% Modeling for Aircraft Engine Run-to-Failure Simulation”, in the
% Proceedings of the 1st International Conference on Prognostics and Health
% Management (PHM08), Denver CO, Oct 2008.

%% Load data
% We explored importing, visualizing and preprocessing the data in the
% UnsupervisedScript, so here we will simply load in the data.
load classificationData

%% Define classification thresholds
% To solve this as a classification problem, we need to define what the
% classes are and where the boundaries are between them. This typically is
% something you cannot do purely from the equipment sensor data. Here we
% drew arbitrary boundaries to create four different classes. We will
% attempt to classify each point as being urgently in need of maintenance,
% or having a short, medium, or long time until maintenance is needed.

catThreshold = [50,125,200];
orderedCategory = {'urgent','short','medium','long'};
fullDataset.TTF = createThresholds(fullDataset,catThreshold,orderedCategory);

% Look at spread of Classes
tabulate(fullDataset.TTF)

% Pull out just the sensor data
sensorData = fullDataset(:,3:end); % ignore Unit and Time variables

%% Visualize all sensor data in categories
% Here we look at a subset of the available sensors. The engines have been
% aligned so that they all fail at time zero and negative values represent
% flights prior to failure. We can see how the sensors change as they
% approach failure and where we have placed the different classes.

figure
for ii = 1:9
    h(ii) = subplot(3,3,ii); %#ok
    scatter(h(ii),fullDataset.Time,fullDataset{:,2+ii},[],fullDataset.TTF, 'filled');
    title(h(ii),fullDataset.Properties.VariableNames{2+ii})
    xlabel(h(ii),'Time')
    set(h(ii), 'CLim', [1 length(catThreshold)+1])
end

%% Rapid preparation for Machine Learning by leveraging App generated codes
% One of the biggest challenges in machine learning is that there is no
% good way to know ahead of time what machine learning algorithm will work
% best until you try them out and see. The Classification Learner App from
% the Statistics and Machine Learning Toolbox makes greatly simplifies this
% workflow. We can explore many different techniques in the app, and in
% this case find that a K-nearest neighbors algorithm seems to work best.

% This command will bring up the app, or you can find it in the apps tab.
% Use the sensorData variable when importing data from the workspace into
% the Classification Learner App
classificationLearner

%% Create training and test sets
% The app will automatically create training and testing data sets when
% working within it. We can also create them ourselves when running outside
% the app.

predictorNames = fullDataset.Properties.VariableNames(3:end-1);
predictors = fullDataset(:, predictorNames);
response = fullDataset.TTF;

% Set up partitions and holdout validation
cvp = cvpartition(response, 'Holdout', 0.2);
Xtrain = predictors(cvp.training,:);
Ytrain = response(cvp.training,:);
disp('Training set');
tabulate(Ytrain)

% Test data
Xtest = predictors(cvp.test,:);
Ytest = response(cvp.test,:);
disp('Test set');
tabulate(Ytest)

%% Easy to develop, evaluate models by leveraging Apps, e.g., k-nearest neighbor
% In our case, k-nearest neighbor outperforms other models based on
% confusion matrix. From the app we can auto-generate MATLAB code to train
% the model, which is copied here.

KNNClassifier = fitcknn(...
    Xtrain, ...
    Ytrain, ...
    'Distance', 'Euclidean', ...
    'Exponent', [], ...
    'NumNeighbors', 5, ...
    'DistanceWeight', 'Equal', ...
    'Standardize', true, ...
    'ClassNames', orderedCategory);

% Predict Response using Predictors in Test dataset
validationPredictions = predict(KNNClassifier, Xtest);
validationPredictions = categorical(validationPredictions, orderedCategory,'Ordinal',true);

% Visualize confusion matrix
addpath(fullfile(pwd,'helperFunctions'))
C_nn = confusionmat(Ytest,validationPredictions);
C_np = fdispConfusion(C_nn, 'K-nearest neighbor (K=5)', orderedCategory);

figure
fheatmap(C_np, orderedCategory, orderedCategory, ...
    '%0.1f%%', ...
    'TickAngle', 45, ...
    'Colormap', 'money', ...
    'Colorbar', true, ...
    'ColorLevels', 5);
title('Confusion Matrix for K-Nearest Neighbor (K=5)');
xlabel('Predicted class');
ylabel('True class');

%% Cost Matrix
% We can use a cost matrix to prioritize certain errors. For example the
% 'urgent' cases are the most important to capture, so we can penalize
% errors when the 'urgent' class is misclassified more than other types of
% errors. However, it is also wasteful to do maintenance well before it is
% needed, so we may add a smaller penalty to when we predict 'urgent' when
% it is acutally 'medium' or 'long'.
CostMatrix = ones(4) - eye(4);
CostMatrix(1,2:4) = 5;
CostMatrix(4,1) = 3;
CostMatrix(3,1) = 2;
disp('Cost Matrix:')
disp(CostMatrix)

%% New model with K=5 and custom cost matrix
% We can use cost matrix to prioritize certain errors. Now we are
% optimizing for the overall impact of our mistakes based on our cost
% matrix rather than just minimizing the raw number of mistakes. We can see
% that this greatly reduces the number of urgent cases that are not
% correctly captured, but also increases the number of short cases that are
% incorrectly classified as urgent. This may actually cause us to make a
% greater number of total mistakes, but if it concentrates those mistakes
% in areas where they are less costly to make then that is preferrable.

% New model with K=5 and custom cost matrix
KNNClassifier5c = fitcknn(...
    Xtrain, ...
    Ytrain, ...
    'Distance', 'Euclidean', ...
    'Exponent', [], ...
    'NumNeighbors', 5, ...
    'DistanceWeight', 'Equal', ...
    'Standardize', true, ...
    'Cost',CostMatrix, ...
    'ClassNames', orderedCategory);

% Predict Response using Predictors in Test dataset
[validationPredictions, validationScores] = predict(KNNClassifier5c, Xtest);
validationPredictions = categorical(validationPredictions, orderedCategory,'Ordinal',true);

% Visualize confusion matrix
addpath(fullfile(pwd,'helperFunctions'))
C_nn5c = confusionmat(Ytest,validationPredictions);
C_np5c = fdispConfusion(C_nn5c, 'k-nearest neighbor (k=5) with CostMatrix', orderedCategory);

figure
fheatmap(C_np5c, orderedCategory, orderedCategory, ...
    '%0.1f%%', ...
    'TickAngle', 45, ...
    'Colormap', 'money', ...
    'Colorbar', true, ...
    'ColorLevels', 5);
title('Confusion Matrix for K-Nearest Neighbor (K=5) with CostMatrix');
xlabel('Predicted class');
ylabel('True class');

##### SOURCE END #####
--></body></html>